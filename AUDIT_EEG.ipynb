{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb44418-2325-4b0c-8024-2daf6837f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Paths to the directories for Set A and Set E data (update with your actual paths)\n",
    "set_a_path = r\"C:\\Users\\tanis\\OneDrive\\Desktop\\audit_EEG\\Z\"  # Path to Set A (Normal)\n",
    "set_e_path = r\"C:\\Users\\tanis\\OneDrive\\Desktop\\audit_EEG\\S\"  # Path to Set E (Epileptic Seizure)\n",
    "\n",
    "# Load Set A (normal) data\n",
    "set_a_data = []\n",
    "for file_name in os.listdir(set_a_path):\n",
    "    file_path = os.path.join(set_a_path, file_name)\n",
    "    data = np.loadtxt(file_path)  # Load each file as an array of 4097 samples\n",
    "    set_a_data.append(data)\n",
    "\n",
    "# Load Set E (epileptic seizure) data\n",
    "set_e_data = []\n",
    "for file_name in os.listdir(set_e_path):\n",
    "    file_path = os.path.join(set_e_path, file_name)\n",
    "    data = np.loadtxt(file_path)  # Load each file as an array of 4097 samples\n",
    "    set_e_data.append(data)\n",
    "\n",
    "# Combine the datasets\n",
    "X = np.array(set_a_data + set_e_data)  # Shape: (200, 4097) if each set has 100 segments\n",
    "y = np.hstack((np.zeros(len(set_a_data)), np.ones(len(set_e_data))))  # Labels: 0 for normal, 1 for seizure\n",
    "\n",
    "# Normalize the data to have zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00650895-15ea-406a-8a7c-ac141191ab3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix Shape: (200, 14)\n"
     ]
    }
   ],
   "source": [
    "import pywt\n",
    "\n",
    "# Function to apply DWT and extract wavelet coefficients\n",
    "def extract_wavelet_coefficients(signal):\n",
    "    # Decompose signal with 'db4' wavelet at level 5\n",
    "    coeffs = pywt.wavedec(signal, wavelet='db4', level=5)\n",
    "    return coeffs\n",
    "\n",
    "# Function to calculate statistical features from wavelet coefficients\n",
    "def extract_wavelet_features(coeffs):\n",
    "    features = []\n",
    "    A5, D5, D4, D3, D2, D1 = coeffs\n",
    "\n",
    "    # 1. Mean of absolute values (captures the average amplitude)\n",
    "    features.append(np.mean(np.abs(A5)))\n",
    "    features.append(np.mean(np.abs(D3)))\n",
    "    features.append(np.mean(np.abs(D4)))\n",
    "    features.append(np.mean(np.abs(D5)))\n",
    "\n",
    "    # 2. Average power (energy of the signal within each sub-band)\n",
    "    features.append(np.sum(np.square(A5)) / len(A5))\n",
    "    features.append(np.sum(np.square(D3)) / len(D3))\n",
    "    features.append(np.sum(np.square(D4)) / len(D4))\n",
    "    features.append(np.sum(np.square(D5)) / len(D5))\n",
    "\n",
    "    # 3. Standard deviation (measures variability in each sub-band)\n",
    "    features.append(np.std(A5))\n",
    "    features.append(np.std(D3))\n",
    "    features.append(np.std(D4))\n",
    "    features.append(np.std(D5))\n",
    "\n",
    "    # 4. Ratio of absolute mean values of adjacent sub-bands (D3/D4 and D4/D5)\n",
    "    features.append(np.mean(np.abs(D3)) / np.mean(np.abs(D4)))\n",
    "    features.append(np.mean(np.abs(D4)) / np.mean(np.abs(D5)))\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "# Apply DWT to each EEG segment and extract features\n",
    "coefficients = [extract_wavelet_coefficients(segment) for segment in X_normalized]\n",
    "X_features = np.array([extract_wavelet_features(coeff) for coeff in coefficients])\n",
    "\n",
    "print(\"Feature Matrix Shape:\", X_features.shape)  # Expected shape: (200, 14) if there are 14 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e288dcec-0e3e-43cf-91d5-a604ea03be6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Reduced Shape: (200, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensions to 10 components with PCA\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X_features)\n",
    "\n",
    "print(\"PCA Reduced Shape:\", X_pca.shape)  # Shape should be (200, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e524db89-13a1-46e6-8bff-bd8889a6ee46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICA Reduced Shape: (200, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "# Reduce dimensions to 10 components with ICA\n",
    "ica = FastICA(n_components=10, random_state=42)\n",
    "X_ica = ica.fit_transform(X_features)\n",
    "\n",
    "print(\"ICA Reduced Shape:\", X_ica.shape)  # Shape should be (200, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e37ac20-064e-462f-89bd-09ae3f953ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Reduced Shape: (200, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Reduce dimensions with LDA (LDA reduces to n_classes - 1, here 1 component since binary classification)\n",
    "lda = LDA(n_components=1)\n",
    "X_lda = lda.fit_transform(X_features, y)\n",
    "\n",
    "print(\"LDA Reduced Shape:\", X_lda.shape)  # Shape should be (200, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73ff72f5-f48b-46a6-ac9a-52cafd3ec59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[31  0]\n",
      " [ 1 28]]\n",
      "Accuracy: 98.333\n",
      "Sensitivity: 96.552\n",
      "Specificity: 100.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Split PCA features for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the SVM\n",
    "svm = SVC(kernel='rbf', gamma='auto')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluation (Confusion Matrix, Sensitivity, Specificity)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Sensitivity (True Positive Rate)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(f\"Accuracy: {accuracy * 100:.3f}\")\n",
    "print(f\"Sensitivity: {sensitivity* 100:.3f}\")\n",
    "print(f\"Specificity: {specificity* 100:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "861a832e-8e7a-4ea8-b63e-6e7d6d73e1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[31  0]\n",
      " [ 1 28]]\n",
      "Accuracy: 98.333\n",
      "Sensitivity: 96.552\n",
      "Specificity: 100.000\n"
     ]
    }
   ],
   "source": [
    "# Split ICA features for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ica, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train SVM\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluation (Confusion Matrix, Sensitivity, Specificity)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Sensitivity (True Positive Rate)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(f'Accuracy: {accuracy * 100:.3f}')\n",
    "print(f\"Sensitivity: {sensitivity* 100:.3f}\")\n",
    "print(f\"Specificity: {specificity* 100:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9de28a8-71df-4144-94bc-3f0d4792502c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[31  0]\n",
      " [ 0 29]]\n",
      "Accuracy: 100.000\n",
      "Sensitivity: 100.000\n",
      "Specificity: 100.000\n"
     ]
    }
   ],
   "source": [
    "# Split LDA features for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lda, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train SVM\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluation (Confusion Matrix, Sensitivity, Specificity)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Sensitivity (True Positive Rate)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(f'Accuracy: {accuracy * 100:.3f}')\n",
    "print(f\"Sensitivity: {sensitivity* 100:.3f}\")\n",
    "print(f\"Specificity: {specificity* 100:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef2c275-14c4-47aa-b4c7-69cf7e3da567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
